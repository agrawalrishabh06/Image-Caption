{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca86752c",
   "metadata": {},
   "source": [
    "# Main Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CgL73kTpZHvm",
   "metadata": {
    "id": "CgL73kTpZHvm"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6w1xE9y3Zz_F",
   "metadata": {
    "id": "6w1xE9y3Zz_F"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "datadir = \"/content/drive/MyDrive/CS_444/DL_project/clip-gpt-captioning/src\"\n",
    "\n",
    "os.chdir(datadir)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xZbNGLIFbHrN",
   "metadata": {
    "id": "xZbNGLIFbHrN"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gPNLIcQQknjA",
   "metadata": {
    "id": "gPNLIcQQknjA"
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b0ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!conda install pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db9844",
   "metadata": {},
   "source": [
    "# Preparing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set constants\n",
    "    SEED = 100\n",
    "    DATA_PATH = os.path.join('data')\n",
    "\n",
    "    # Set random seed\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load CLIP model and processor\n",
    "    preprocessor = CLIPProcessor.from_pretrained('openai/clip-vit-large-patch14')\n",
    "    model = CLIPModel.from_pretrained('openai/clip-vit-large-patch14').vision_model.to(device)\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(os.path.join(DATA_PATH, 'raw', 'results.csv'), sep='|')\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "    df = df.drop(['comment_number'], axis=1)\n",
    "\n",
    "    # get every 5 elemeFnt of the df (5 captions per image) and save image name with corresponding captions\n",
    "    ds = [(img_name, df[df['image_name'] == img_name]['comment'].values) for img_name, _ in df[0::5].to_numpy()]\n",
    "\n",
    "    # Based on loaded dataset, create a list of (image name, image embedding, caption) tuples\n",
    "    results = []\n",
    "    loop = tqdm(ds, total=len(ds), position=0, leave=True)\n",
    "    for img_name, cap in loop:\n",
    "        try:\n",
    "            img = Image.open(os.path.join(DATA_PATH, 'raw', 'flickr10k_images', img_name))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                img_prep = preprocessor(images=img, return_tensors='pt').to(device)\n",
    "                \n",
    "                img_features = model(**img_prep)\n",
    "                img_features = img_features.pooler_output\n",
    "                img_features = img_features.squeeze()\n",
    "                img_features = img_features.numpy()\n",
    "\n",
    "            for c in cap:\n",
    "                results.append((img_name, img_features, c[1:])) # because of the separator there is a space at the beginning of the caption\n",
    "                \n",
    "        except:\n",
    "            print(f'Lack of image {img_name}')\n",
    "\n",
    "    # save data into pickle file\n",
    "    # img_name, img_features, caption\n",
    "    with open(os.path.join(DATA_PATH, 'processed', 'dataset.pkl'), 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64cd48",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da1a86",
   "metadata": {
    "id": "75da1a86"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import wandb\n",
    "from data import MiniFlickrDataset, get_loader\n",
    "from model import Net, Trainer\n",
    "from utils import ConfigS, ConfigL, LRWarmup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "khVUwi3FYrMx",
   "metadata": {
    "id": "khVUwi3FYrMx"
   },
   "outputs": [],
   "source": [
    "checkpoint_name = 'model_train.pt'\n",
    "size = 'S'\n",
    "\n",
    "config = ConfigL() if size.upper() else ConfigS()\n",
    "\n",
    "# set seed\n",
    "random.seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed(config.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "\n",
    "    dataset = MiniFlickrDataset(os.path.join('data', 'processed', 'data.pkl'))\n",
    "\n",
    "    config.train_size = int(config.train_size * len(dataset))\n",
    "    config.val_size = int(config.val_size * len(dataset))\n",
    "    config.test_size = len(dataset) - config.train_size - config.val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [config.train_size, config.val_size, config.test_size])\n",
    "\n",
    "    train_loader = get_loader(\n",
    "        train_dataset, \n",
    "        bs_exp=config.batch_size_exp if is_cuda else 2, \n",
    "        shuffle=True, \n",
    "        num_workers=config.num_workers if is_cuda else 0,\n",
    "        pin_memory=is_cuda\n",
    "    )\n",
    "\n",
    "    valid_loader = get_loader(\n",
    "        val_dataset, \n",
    "        bs_exp=config.batch_size_exp if is_cuda else 2, \n",
    "        shuffle=False, \n",
    "        num_workers=config.num_workers if is_cuda else 0,\n",
    "        pin_memory=is_cuda\n",
    "    )\n",
    "\n",
    "    model = Net(\n",
    "        clip_model=config.clip_model,\n",
    "        text_model=config.text_model,\n",
    "        ep_len=config.ep_len,\n",
    "        num_layers=config.num_layers, \n",
    "        n_heads=config.n_heads, \n",
    "        forward_expansion=config.forward_expansion, \n",
    "        dropout=config.dropout, \n",
    "        max_len=config.max_len,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "    warmup = LRWarmup(epochs=config.epochs, max_lr=config.lr, k=config.k)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, warmup.lr_warmup)\n",
    "    scaler = torch.cuda.amp.GradScaler()    \n",
    "\n",
    "    ckp_path = os.path.join(config.weights_dir,checkpoint_name)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scaler=scaler,\n",
    "        scheduler=scheduler,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        test_dataset=test_dataset,\n",
    "        test_path=os.path.join('data', 'raw', 'flickr8k_images'),\n",
    "        ckp_path=ckp_path,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # build train model process with experiment tracking from wandb\n",
    "    wandb.init(project='clipXgpt2 captioner', config=config.__dict__)\n",
    "    wandb.watch(trainer.model, log='all')\n",
    "    for epoch in range(trainer.epoch, config.epochs):\n",
    "        trainer.train_epoch()\n",
    "        trainer.valid_epoch()\n",
    "        trainer.test_step()\n",
    "\n",
    "        metadata = trainer.get_training_data()\n",
    "\n",
    "        # log loss to wandb\n",
    "        wandb.log({\n",
    "            'train_loss/loss': metadata['train_loss'][-1],\n",
    "            'valid_loss/loss': metadata['valid_loss'][-1],\n",
    "            'lr': metadata['lr'],\n",
    "            'examples': wandb.Image(metadata['examples']),\n",
    "        })\n",
    "\n",
    "        if not os.path.exists(config.weights_dir):\n",
    "            os.makedirs(config.weights_dir)\n",
    "\n",
    "        if (epoch + 1) % 6 == 0:\n",
    "            trainer.save_ckp(os.path.join(config.weights_dir, f'epoch_{epoch + 1}.pt'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
